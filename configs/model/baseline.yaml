name: "baseline"

defaults:
  - _self_

# Global settings that can be overridden from command line
freeze: true
batch_norm: false
current_stage: 1
fading: true

# Static architectural parameters
n_filter: 120
n_stages: 6
n_channels: 21
n_classes: 2

generator:
  _target_: src.models.components.generators.multi_channel_upscale.Generator
  n_filter: ${model.n_filter}
  n_stages: ${model.n_stages}
  n_channels: ${model.n_channels}
  n_classes: ${model.n_classes}
  freeze: ${model.freeze}
  batch_norm: ${model.batch_norm}
  current_stage: ${model.current_stage}
  fading: ${model.fading}
  latent_dim: 210
  embedding_dim: 100

critic:
  _target_: src.models.components.critics.baseline.Critic
  n_filter: ${model.n_filter}
  n_stages: ${model.n_stages}
  n_channels: ${model.n_channels}
  n_classes: ${model.n_classes}
  freeze: ${model.freeze}
  batch_norm: ${model.batch_norm}
  current_stage: ${model.current_stage}
  fading: ${model.fading}

spectral_critic:
  _target_: src.models.components.SpectralCritic
  n_filter: ${model.n_filter}
  n_stages: ${model.n_stages}
  n_channels: ${model.n_channels}
  n_classes: ${model.n_classes}
  freeze: ${model.freeze}
  batch_norm: ${model.batch_norm}
  current_stage: ${model.current_stage}
  fading: ${model.fading}

gan:
  _target_: src.models.GAN
  n_epochs_critic: 5
  alpha: 1
  beta: 0.2
  lambda_gp: 10

optimizer:
  name: "adam"
  lr_generator: 0.001
  lr_critic: 0.001
  betas: [0.0, 0.999]
